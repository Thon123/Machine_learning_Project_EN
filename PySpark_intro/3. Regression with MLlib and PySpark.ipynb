{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with MLlib and PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Algorithms iny Pyspark:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear regression\n",
    "2. Decision tree regression\n",
    "3. Random forest regression\n",
    "4. Gradit-boosted tree regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This will be very shor notebook, witout any analysis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Data</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Creating our session*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Regression\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Mandatory imports.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "#New one - for multicolinearity checking\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Dataset**\n",
    "\n",
    "This is a dataset containing housing pricing data for California. Each row of data represents the median statistics for a block (eg. median income, median age of house, etc.). You could this data in a number of ways, but we will use it to predict the median house value. \n",
    "\n",
    "**About this dataset **\n",
    "1. longitude: A measure of how far west a house is; a higher value is farther west\n",
    "2. latitude: A measure of how far north a house is; a higher value is farther north\n",
    "3. housingMedianAge: Median age of a house within a block; a lower number is a newer building\n",
    "4. totalRooms: Total number of rooms within a block\n",
    "5. totalBedrooms: Total number of bedrooms within a block\n",
    "6. population: Total number of people residing within a block\n",
    "7. households: Total number of households, a group of people residing within a home unit, for a block\n",
    "8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "9. medianHouseValue: Median house value for households within a block (measured in US Dollars)\n",
    "10. oceanProximity: Location of the house w.r.t ocean/sea\n",
    "\n",
    "**Source:** https://www.kaggle.com/camnugent/california-housing-prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"Datasets/\"\n",
    "df = spark.read.csv(path+'housing.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Droping nan rows.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20433"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.na.drop()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Data preparation</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Firstly assinging our input and dependent.We will use only 4 columns here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = ['total_bedrooms','population','households','median_income']\n",
    "dependent_var = 'median_house_value'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Renaming our depndent to a label.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed = df.withColumnRenamed(dependent_var,'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Just checking if all of my label rows are numberic and if not changing to float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if str(renamed.schema['label'].dataType) != 'IntegerType':\n",
    "    renamed = renamed.withColumn(\"label\", renamed[\"label\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now I am converting all string data types to a numeric one.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_inputs = []\n",
    "string_inputs = []\n",
    "for column in input_columns:\n",
    "    if str(renamed.schema[column].dataType) == 'StringType':\n",
    "        new_col_name = column+\"_num\"\n",
    "        string_inputs.append(new_col_name)\n",
    "    else:\n",
    "        numeric_inputs.append(column)\n",
    "        indexed = renamed\n",
    "        \n",
    "if len(string_inputs) != 0: \n",
    "    for column in input_columns:\n",
    "        if str(renamed.schema[column].dataType) == 'StringType':\n",
    "            indexer = StringIndexer(inputCol=column, outputCol=column+\"_num\") \n",
    "            indexed = indexer.fit(renamed).transform(renamed)\n",
    "        else:\n",
    "            indexed = renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total_bedrooms', 'population', 'households', 'median_income']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outliers - same old skew function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_bedrooms has been treated for positive (right) skewness. (skew =) 3.4592923587675024 )\n",
      "population has been treated for positive (right) skewness. (skew =) 4.959652416875933 )\n",
      "households has been treated for positive (right) skewness. (skew =) 3.4135995729616138 )\n",
      "median_income has been treated for positive (right) skewness. (skew =) 1.6444361858367003 )\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d = {}\n",
    "\n",
    "for col in numeric_inputs: \n",
    "    d[col] = indexed.approxQuantile(col,[0.01,0.99],0.25) \n",
    "for col in numeric_inputs:\n",
    "    skew = indexed.agg(skewness(indexed[col])).collect() #check for skewness\n",
    "    skew = skew[0][0]\n",
    "    # This function will floor, cap and then log+1 (just in case there are 0 values)\n",
    "    if skew > 1:\n",
    "        indexed = indexed.withColumn(col, \\\n",
    "        log(when(df[col] < d[col][0],d[col][0])\\\n",
    "        .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "        .otherwise(indexed[col] ) +1).alias(col))\n",
    "        print(col+\" has been treated for positive (right) skewness. (skew =)\",skew,\")\")\n",
    "    elif skew < -1:\n",
    "        indexed = indexed.withColumn(col, \\\n",
    "        exp(when(df[col] < d[col][0],d[col][0])\\\n",
    "        .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "        .otherwise(indexed[col] )).alias(col))\n",
    "        print(col+\" has been treated for negative (left) skewness. (skew =\",skew,\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = numeric_inputs + string_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total_bedrooms', 'population', 'households', 'median_income']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
    "final_data = assembler.transform(indexed).select('features','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|   label|\n",
      "+--------------------+--------+\n",
      "|[4.86753445045558...|452600.0|\n",
      "|[7.00940893270863...|358500.0|\n",
      "|[5.25227342804663...|352100.0|\n",
      "|[5.46383180502561...|341300.0|\n",
      "|[5.63835466933374...|342200.0|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Multicollinearity check</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Just checking if some of our pairs are not correlated too much.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "pearsonCorr = Correlation.corr(final_data, 'features', 'pearson').collect()[0][0]\n",
    "array = pearsonCorr.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      " \n",
      "0.8975232138344982\n",
      " \n",
      "0.9745926155657936\n",
      "0.8975232138344982\n",
      " \n",
      "1.0\n",
      " \n",
      "0.932189718732677\n",
      "0.9745926155657936\n",
      " \n",
      "0.932189718732677\n",
      " \n",
      "1.0\n",
      "0.014403796002467354\n",
      " \n",
      "0.0317896708855436\n",
      " \n",
      "0.046944012890930975\n"
     ]
    }
   ],
   "source": [
    "for item in array:\n",
    "    print(item[0])\n",
    "    print(\" \")\n",
    "    print(item[1])\n",
    "    print(\" \")\n",
    "    print(item[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some correlated data here. For linear regreesion is a good solution to delete some of them, for other estimators it is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Linear regressor</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLinear Regression Model Summary WITH cross validation:\u001b[0m\n",
      " \n",
      "Coefficient Standard Errors: \n",
      "+--------------+------------------+\n",
      "|feature       |coeff std error   |\n",
      "+--------------+------------------+\n",
      "|households    |5216.668975931509 |\n",
      "|total_bedrooms|4280.169699530648 |\n",
      "|population    |2588.08643911679  |\n",
      "|median_income |1929.0096209205208|\n",
      "+--------------+------------------+\n",
      "\n",
      "None\n",
      " \n",
      "P Values: \n",
      "+--------------+--------------------+\n",
      "|feature       |P-Value             |\n",
      "+--------------+--------------------+\n",
      "|total_bedrooms|0.015851289930112644|\n",
      "|population    |0.0                 |\n",
      "|median_income |0.0                 |\n",
      "|households    |0.0                 |\n",
      "+--------------+--------------------+\n",
      "\n",
      "None\n",
      " \n",
      "RMSE: 81873.67532455365\n"
     ]
    }
   ],
   "source": [
    "regressor = LinearRegression()\n",
    "\n",
    "#Now train with cross val\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "             .addGrid(regressor.maxIter, [10, 15]) \\\n",
    "             .addGrid(regressor.regParam, [0.1, 0.01]) \\\n",
    "             .build())\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "\n",
    "#Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(estimator=regressor,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2) # 3 is best practice\n",
    "\n",
    "print('\\033[1m' + \"Linear Regression Model Summary WITH cross validation:\"+ '\\033[0m')\n",
    "print(\" \")\n",
    "# Run cross validations\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Extract Best model\n",
    "LR_BestModel = fitModel.bestModel\n",
    "\n",
    "# Get Model Summary Statistics\n",
    "# ModelSummary = fitModel.bestModel.summary\n",
    "ModelSummary = LR_BestModel.summary\n",
    "print(\"Coefficient Standard Errors: \")\n",
    "coeff_ste = ModelSummary.coefficientStandardErrors\n",
    "result = spark.createDataFrame(zip(input_columns,coeff_ste), schema=['feature','coeff std error'])\n",
    "print(result.orderBy(result[\"coeff std error\"].desc()).show(truncate=False))\n",
    "print(\" \")\n",
    "print(\"P Values: \") \n",
    "# Then zip with input_columns list and create a df\n",
    "pvalues = ModelSummary.pValues\n",
    "result = spark.createDataFrame(zip(input_columns,pvalues), schema=['feature','P-Value'])\n",
    "print(result.orderBy(result[\"P-Value\"].desc()).show(truncate=False))\n",
    "print(\" \")\n",
    "\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "ModelPredictions = fitModel.transform(test)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "test_results = evaluator.evaluate(ModelPredictions)\n",
    "print('RMSE:', test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Decision Trees</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[1m Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+--------------+--------------------+\n",
      "|feature       |score               |\n",
      "+--------------+--------------------+\n",
      "|median_income |0.9412743117388724  |\n",
      "|households    |0.025483948811760478|\n",
      "|population    |0.025157915442438228|\n",
      "|total_bedrooms|0.00808382400692876 |\n",
      "+--------------+--------------------+\n",
      "\n",
      "None\n",
      "81443.11956252148\n"
     ]
    }
   ],
   "source": [
    "regressor = DecisionTreeRegressor()\n",
    "\n",
    "# Build your parameter grid\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "                         .addGrid(regressor.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "                         .addGrid(regressor.maxBins, [10, 20, 40]) \\\n",
    "                         .build())\n",
    "\n",
    "#Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(estimator=regressor,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(metricName=\"rmse\"),\n",
    "                          numFolds=2) # 3 is best practice\n",
    "\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Get Best Model\n",
    "DT_BestModel = fitModel.bestModel\n",
    "\n",
    "# FEATURE IMPORTANCES\n",
    "# Estimate of the importance of each feature.\n",
    "# Each feature’s importance is the average of its importance across all trees \n",
    "# in the ensemble The importance vector is normalized to sum to 1. \n",
    "print(\" \")\n",
    "print('\\033[1m' + \" Feature Importances\"+ '\\033[0m')\n",
    "print(\"(Scores add up to 1)\")\n",
    "print(\"Lowest score is the least important\")\n",
    "print(\" \")\n",
    "DT_featureImportances = DT_BestModel.featureImportances.toArray()\n",
    "# Convert from numpy array to list\n",
    "imp_scores = []\n",
    "for x in DT_featureImportances:\n",
    "    imp_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
    "print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
    "\n",
    "# Make predictions.\n",
    "# PySpark will automatically use the best model when you call fitmodel\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "\n",
    "# And then apply it your predictions dataframe\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Random Forest Regressor</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[1m Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+--------------+--------------------+\n",
      "|feature       |score               |\n",
      "+--------------+--------------------+\n",
      "|median_income |0.7625897722359528  |\n",
      "|population    |0.1128317787702994  |\n",
      "|households    |0.06492867479303441 |\n",
      "|total_bedrooms|0.059649774200713365|\n",
      "+--------------+--------------------+\n",
      "\n",
      "None\n",
      "76006.20702902295\n"
     ]
    }
   ],
   "source": [
    "regressor = RandomForestRegressor()\n",
    "\n",
    "# Add parameters of your choice here:\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "                           .addGrid(regressor.maxDepth, [2, 5, 10])\n",
    "                           .addGrid(regressor.maxBins, [5, 10, 20])\n",
    "               .addGrid(regressor.numTrees, [5, 20])\n",
    "             .build())\n",
    "\n",
    "#Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(estimator=regressor,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2) # 3 is best practice\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Get Best Model\n",
    "RF_BestModel = fitModel.bestModel\n",
    "\n",
    "# FEATURE IMPORTANCES\n",
    "# Estimate of the importance of each feature.\n",
    "# Each feature’s importance is the average of its importance across all trees \n",
    "# in the ensemble The importance vector is normalized to sum to 1. \n",
    "print(\" \")\n",
    "print('\\033[1m' + \" Feature Importances\"+ '\\033[0m')\n",
    "print(\"(Scores add up to 1)\")\n",
    "print(\"Lowest score is the least important\")\n",
    "print(\" \")\n",
    "RF_featureImportances = RF_BestModel.featureImportances.toArray()\n",
    "# Convert from numpy array to list\n",
    "imp_scores = []\n",
    "for x in RF_featureImportances:\n",
    "    imp_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
    "print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
    "\n",
    "# Make predictions.\n",
    "# PySpark will automatically use the best model when you call fitmodel\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "# And then apply it your predictions dataframe\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Gradient Boosted Regressor</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[1m Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+--------------+-------------------+\n",
      "|feature       |score              |\n",
      "+--------------+-------------------+\n",
      "|median_income |0.4432783006540673 |\n",
      "|population    |0.26028531900622015|\n",
      "|households    |0.21660222723099073|\n",
      "|total_bedrooms|0.07983415310872183|\n",
      "+--------------+-------------------+\n",
      "\n",
      "None\n",
      "76193.25100108575\n"
     ]
    }
   ],
   "source": [
    "regressor = GBTRegressor()\n",
    "\n",
    "# Add parameters of your choice here:\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "                           .addGrid(regressor.maxDepth, [2, 5, 10])\n",
    "                           .addGrid(regressor.maxBins, [5, 10, 20])\n",
    "                           .build())\n",
    "\n",
    "#Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(estimator=regressor,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2) # 3 is best practice\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Get Best Model\n",
    "GBT_BestModel = fitModel.bestModel\n",
    "\n",
    "# FEATURE IMPORTANCES\n",
    "# Estimate of the importance of each feature.\n",
    "# Each feature’s importance is the average of its importance across all trees \n",
    "# in the ensemble The importance vector is normalized to sum to 1. \n",
    "print(\" \")\n",
    "print('\\033[1m' + \" Feature Importances\"+ '\\033[0m')\n",
    "print(\"(Scores add up to 1)\")\n",
    "print(\"Lowest score is the least important\")\n",
    "print(\" \")\n",
    "GBT_featureImportances = GBT_BestModel.featureImportances.toArray()\n",
    "# Convert from numpy array to list\n",
    "imp_scores = []\n",
    "for x in GBT_featureImportances:\n",
    "    imp_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
    "print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
    "\n",
    "# Make predictions.\n",
    "# PySpark will automatically use the best model when you call fitmodel\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "# And then apply it your predictions dataframe\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------------+\n",
      "|            features|   label|        prediction|\n",
      "+--------------------+--------+------------------+\n",
      "|[1.38629436111989...|250000.0|227978.71826770186|\n",
      "|[1.60943791243410...|500001.0| 377555.7430468508|\n",
      "|[1.60943791243410...| 80000.0|178535.08350059908|\n",
      "|[1.79175946922805...|118800.0|131510.85703766922|\n",
      "|[1.79175946922805...|137500.0|         459775.85|\n",
      "|[1.94591014905531...| 67500.0|131510.85703766922|\n",
      "|[2.07944154167983...|500001.0| 377555.7430468508|\n",
      "|[2.07944154167983...|162500.0|227978.71826770186|\n",
      "|[2.19722457733621...| 71300.0|147682.34609985608|\n",
      "|[2.19722457733621...|450000.0|131510.85703766922|\n",
      "|[2.19722457733621...|150000.0| 142157.6140057303|\n",
      "|[2.19722457733621...|225000.0| 377555.7430468508|\n",
      "|[2.30258509299404...|375000.0|119101.78234203234|\n",
      "|[2.39789527279837...| 42500.0|119101.78234203234|\n",
      "|[2.39789527279837...|225000.0|119101.78234203234|\n",
      "|[2.39789527279837...|137500.0|164527.62629639663|\n",
      "|[2.48490664978800...| 67500.0| 187066.9152274222|\n",
      "|[2.48490664978800...| 75000.0|147682.34609985608|\n",
      "|[2.56494935746153...| 75000.0| 187066.9152274222|\n",
      "|[2.56494935746153...| 67500.0|178535.08350059908|\n",
      "+--------------------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = RF_BestModel.transform(test)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"><b>BeAware:</b>only fo educational purposes, quite a good reference</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The End***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
